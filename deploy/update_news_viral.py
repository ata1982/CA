#!/usr/bin/env python3
"""
Viral News Update System
100+ categories, SNS trends, gossip, YouTube integration, real-time viral detection
"""

import os
import sys
import json
import logging
import asyncio
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List

# Add backend directory to path
sys.path.insert(0, '/home/ubuntu/news-ai-site/backend')

from deepseek_processor import DeepSeekProcessor
from extended_news_fetcher import ExtendedNewsFetcher
from viral_frontend import generate_viral_frontend

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Path(tempfile.gettempdir()) / 'news_update_viral.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ViralNewsUpdater:
    def __init__(self, public_dir='/var/www/html'):
        self.public_dir = Path(public_dir)
        self.public_dir.mkdir(exist_ok=True)
        
        self.processor = DeepSeekProcessor()
        self.fetcher = ExtendedNewsFetcher()
        
        # æ›´æ–°é–“éš”è¨­å®š
        self.update_interval = 180  # 3åˆ†é–“éš”
        self.max_articles = 50      # æœ€å¤§è¨˜äº‹æ•°
        
    async def process_viral_news(self):
        """
        ãƒã‚¤ãƒ©ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å‡¦ç†ãƒ¡ã‚¤ãƒ³é–¢æ•°
        """
        try:
            logger.info("ğŸ”¥ Starting viral news update process...")
            
            # 1. æ‹¡å¼µãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†
            logger.info("ğŸ“¡ Fetching from 100+ sources...")
            raw_articles = await self.fetcher.fetch_all_extended_feeds(max_per_category=2)
            
            if not raw_articles:
                logger.warning("No articles fetched, using fallback")
                raw_articles = self._generate_fallback_articles()
            
            # 2. ãƒã‚¤ãƒ©ãƒ«ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            sorted_articles = sorted(raw_articles, key=lambda x: x.get('viral_score', 0), reverse=True)
            top_articles = sorted_articles[:self.max_articles]
            
            logger.info(f"ğŸ“Š Processing top {len(top_articles)} viral articles...")
            
            # 3. DeepSeekã§åˆ†æï¼ˆé«˜ã‚¹ã‚³ã‚¢è¨˜äº‹å„ªå…ˆï¼‰
            analyzed_articles = []
            
            for i, article in enumerate(top_articles[:20]):  # ä¸Šä½20è¨˜äº‹ã®ã¿åˆ†æ
                try:
                    logger.info(f"ğŸ¤– Analyzing article {i+1}/20: {article['title'][:50]}...")
                    
                    # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ç‚ä¸Šç³»ã¯ç‰¹åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨
                    if self._is_trend_article(article):
                        analyzed = await self._analyze_trend_article(article)
                    else:
                        analyzed = self.processor.analyze_article(article)
                    
                    analyzed_articles.append(analyzed)
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Error analyzing article: {str(e)}")
                    analyzed_articles.append(article)  # åˆ†æå¤±æ•—æ™‚ã¯å…ƒè¨˜äº‹ã‚’ãã®ã¾ã¾
                    continue
            
            # æœªåˆ†æè¨˜äº‹ã‚‚è¿½åŠ ï¼ˆåˆ†æãªã—ï¼‰
            analyzed_articles.extend(top_articles[20:])
            
            # 4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            await self._save_viral_data(analyzed_articles)
            
            # 5. ãƒã‚¤ãƒ©ãƒ«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”Ÿæˆ
            html_content = self._generate_viral_html(analyzed_articles)
            await self._save_html(html_content)
            
            # 6. çµ±è¨ˆæƒ…å ±å‡ºåŠ›
            self._log_viral_stats(analyzed_articles)
            
            logger.info(f"âœ… Viral news update completed. Processed {len(analyzed_articles)} articles.")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Fatal error in viral news update: {str(e)}")
            raise
        
        finally:
            await self.fetcher.close()
            self.processor.close()
    
    def _is_trend_article(self, article: Dict) -> bool:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ç‚ä¸Šç³»è¨˜äº‹ã®åˆ¤å®š
        """
        category = article.get('category', '').lower()
        platform = article.get('platform', '').lower()
        viral_score = article.get('viral_score', 0)
        
        trend_categories = ['sns_trend', 'gossip', 'youtube_trend']
        trend_platforms = ['twitter', 'youtube', 'tiktok']
        
        return (category in trend_categories or 
                platform in trend_platforms or 
                viral_score >= 600)
    
    async def _analyze_trend_article(self, article: Dict) -> Dict:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰è¨˜äº‹ã®ç‰¹åˆ¥åˆ†æ
        """
        try:
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""
            ä»¥ä¸‹ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒã‚¤ãƒ©ãƒ«è¨˜äº‹ã‚’åˆ†æã—ã¦ã€JSONå½¢å¼ã§çµæœã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
            
            è¨˜äº‹æƒ…å ±:
            - ã‚¿ã‚¤ãƒˆãƒ«: {article.get('title', '')}
            - ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {article.get('platform', 'unknown')}
            - ã‚½ãƒ¼ã‚¹: {article.get('source', 'unknown')}
            - ãƒã‚¤ãƒ©ãƒ«ã‚¹ã‚³ã‚¢: {article.get('viral_score', 0)}
            - å†…å®¹: {article.get('content', '')}
            - ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {article.get('trend_keyword', '')}
            
            ä»¥ä¸‹ã®é …ç›®ã‚’å«ã‚€JSONã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
            1. title_ja: æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆã‚­ãƒ£ãƒƒãƒãƒ¼ã ãŒèª‡å¼µã—ãªã„ã€30æ–‡å­—ä»¥å†…ï¼‰
            2. summary: 80-100æ–‡å­—ã®æ—¥æœ¬èªè¦ç´„
            3. trend_analysis: ãªãœãƒˆãƒ¬ãƒ³ãƒ‰ã«ãªã£ã¦ã„ã‚‹ã‹ã®åˆ†æ
            4. viral_potential: ãƒã‚¤ãƒ©ãƒ«æ€§ã®è©•ä¾¡ï¼ˆ1-10ï¼‰
            5. controversy_level: è«–äº‰åº¦ï¼ˆ1-10ï¼‰
            6. social_impact: ç¤¾ä¼šçš„å½±éŸ¿åº¦ã®èª¬æ˜
            7. keywords: é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3-5å€‹
            8. fact_check: äº‹å®Ÿç¢ºèªæ¸ˆã¿ã®éƒ¨åˆ†
            9. speculation: æ¨æ¸¬ãƒ»å™‚ã®éƒ¨åˆ†
            10. target_audience: ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤
            
            å¿…ãšJSONå½¢å¼ã®ã¿ã§è¿”ç­”ã—ã¦ãã ã•ã„ã€‚
            """
            
            response = self.processor.client.post(
                self.processor.api_url,
                json={
                    "model": self.processor.model,
                    "messages": [
                        {"role": "system", "content": "ã‚ãªãŸã¯SNSãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.5,
                    "max_tokens": 1000
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                try:
                    # ã‚ˆã‚Šå¼·å›ºãªJSONæŠ½å‡º
                    from deepseek_processor import DeepSeekProcessor
                    processor_instance = DeepSeekProcessor()
                    cleaned_content = processor_instance._extract_json_from_response(content)
                    analysis = json.loads(cleaned_content)
                    
                    # å…ƒè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã¨åˆ†æçµæœã‚’çµ±åˆ
                    return {
                        **article,
                        "trend_analysis": {
                            "title_ja": analysis.get("title_ja", article.get("title", "")),
                            "summary": analysis.get("summary", ""),
                            "trend_reason": analysis.get("trend_analysis", ""),
                            "viral_potential": analysis.get("viral_potential", 5),
                            "controversy_level": analysis.get("controversy_level", 1),
                            "social_impact": analysis.get("social_impact", ""),
                            "keywords": analysis.get("keywords", []),
                            "fact_check": analysis.get("fact_check", ""),
                            "speculation": analysis.get("speculation", ""),
                            "target_audience": analysis.get("target_audience", ""),
                            "analyzed_at": datetime.utcnow().isoformat()
                        }
                    }
                    
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse trend analysis JSON: {content}")
                    return self._get_fallback_trend_analysis(article)
            else:
                logger.error(f"Trend analysis API error: {response.status_code}")
                return self._get_fallback_trend_analysis(article)
                
        except Exception as e:
            logger.error(f"Trend analysis error: {str(e)}")
            return self._get_fallback_trend_analysis(article)
    
    def _get_fallback_trend_analysis(self, article: Dict) -> Dict:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        return {
            **article,
            "trend_analysis": {
                "title_ja": article.get("title", "")[:30],
                "summary": article.get("content", "")[:100],
                "trend_reason": "APIæ¥ç¶šã‚¨ãƒ©ãƒ¼ã®ãŸã‚è©³ç´°åˆ†æä¸å¯",
                "viral_potential": 5,
                "controversy_level": 1,
                "social_impact": "åˆ†æä¸­",
                "keywords": [],
                "fact_check": "æœªç¢ºèª",
                "speculation": "æƒ…å ±åé›†ä¸­",
                "target_audience": "ä¸€èˆ¬",
                "analyzed_at": datetime.utcnow().isoformat()
            }
        }
    
    async def _save_viral_data(self, articles: List[Dict]):
        """
        ãƒã‚¤ãƒ©ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        """
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ä¿å­˜
        data = {
            "last_updated": datetime.utcnow().isoformat(),
            "update_interval": self.update_interval,
            "article_count": len(articles),
            "viral_articles": len([a for a in articles if a.get('viral_score', 0) >= 800]),
            "trend_articles": len([a for a in articles if a.get('viral_score', 0) >= 400]),
            "sources_count": len(set(a.get('source', '') for a in articles)),
            "platforms": list(set(a.get('platform', '') for a in articles if a.get('platform'))),
            "categories": list(set(a.get('category', '') for a in articles)),
            "articles": articles
        }
        
        json_path = self.public_dir / 'viral_data.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Saved viral data to {json_path}")
    
    def _generate_viral_html(self, articles: List[Dict]) -> str:
        """
        ãƒã‚¤ãƒ©ãƒ«HTMLç”Ÿæˆ
        """
        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        trending_keywords = self._extract_trending_keywords(articles)
        
        # ãƒã‚¤ãƒ©ãƒ«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”Ÿæˆ
        return generate_viral_frontend(articles, trending_keywords)
    
    def _extract_trending_keywords(self, articles: List[Dict]) -> List[str]:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        """
        keywords = {}
        
        for article in articles:
            # ãƒã‚¤ãƒ©ãƒ«ã‚¹ã‚³ã‚¢ã«å¿œã˜ã¦é‡ã¿ä»˜ã‘
            weight = max(1, article.get('viral_score', 0) // 100)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            title = article.get('title', '')
            
            # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            import re
            jp_words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{2,}|[ä¸€-é¾¯]{2,}', title)
            for word in jp_words:
                keywords[word] = keywords.get(word, 0) + weight
            
            # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            en_words = re.findall(r'[A-Za-z]{3,}', title)
            for word in en_words:
                keywords[word.lower()] = keywords.get(word.lower(), 0) + weight
            
            # æ—¢å­˜ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if article.get('trend_keyword'):
                keywords[article['trend_keyword']] = keywords.get(article['trend_keyword'], 0) + weight * 3
        
        # ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿”ã™
        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:15]]
    
    async def _save_html(self, html_content: str):
        """
        HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        """
        html_path = self.public_dir / 'index.html'
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"ğŸ’¾ Saved HTML to {html_path}")
    
    def _log_viral_stats(self, articles: List[Dict]):
        """
        ãƒã‚¤ãƒ©ãƒ«çµ±è¨ˆã®ãƒ­ã‚°å‡ºåŠ›
        """
        stats = {
            'total': len(articles),
            'viral': len([a for a in articles if a.get('viral_score', 0) >= 800]),
            'trending': len([a for a in articles if a.get('viral_score', 0) >= 400]),
            'youtube': len([a for a in articles if a.get('platform') == 'youtube']),
            'gossip': len([a for a in articles if 'gossip' in a.get('category', '')]),
            'sources': len(set(a.get('source', '') for a in articles)),
            'avg_viral_score': sum(a.get('viral_score', 0) for a in articles) / len(articles) if articles else 0
        }
        
        logger.info("ğŸ“Š Viral News Statistics:")
        logger.info(f"   Total articles: {stats['total']}")
        logger.info(f"   ğŸ”¥ Viral (800+): {stats['viral']}")
        logger.info(f"   ğŸ“ˆ Trending (400+): {stats['trending']}")
        logger.info(f"   ğŸ“º YouTube: {stats['youtube']}")
        logger.info(f"   ğŸ’¬ Gossip: {stats['gossip']}")
        logger.info(f"   ğŸ“¡ Sources: {stats['sources']}")
        logger.info(f"   ğŸ“Š Avg Viral Score: {stats['avg_viral_score']:.1f}")
    
    def _generate_fallback_articles(self) -> List[Dict]:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨˜äº‹ç”Ÿæˆ
        """
        return [
            {
                "id": "fallback_viral",
                "title": "ğŸ”¥ ãƒã‚¤ãƒ©ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒé–‹å§‹",
                "content": "100ã‚«ãƒ†ã‚´ãƒªä»¥ä¸Šã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒã‚¤ãƒ©ãƒ«ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰è¨˜äº‹ã‚’åé›†ãƒ»åˆ†æã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãŒç¨¼åƒã—ã¾ã—ãŸã€‚",
                "source": "Viral News System",
                "language": "ja",
                "category": "system",
                "viral_score": 500,
                "reliability_score": 0.9,
                "sensitive_level": 1,
                "published": datetime.utcnow().isoformat(),
                "platform": "system"
            }
        ]

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
async def main():
    """
    éåŒæœŸãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    try:
        updater = ViralNewsUpdater()
        await updater.process_viral_news()
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal error: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    # éåŒæœŸå®Ÿè¡Œ
    asyncio.run(main())