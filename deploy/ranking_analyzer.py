#!/usr/bin/env python3
"""
News Ranking Analyzer
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®äººæ°—è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’åˆ†æã—ã€ãƒã‚¤ãƒ©ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
"""

import os
import sys
import json
import logging
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import re

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    import httpx
    from bs4 import BeautifulSoup
except ImportError:
    httpx = None
    BeautifulSoup = None

logger = logging.getLogger(__name__)

class NewsRankingAnalyzer:
    def __init__(self):
        self.news_sites = {
            'yahoo_news': {
                'url': 'https://news.yahoo.co.jp/ranking/access/news',
                'ranking_selector': '.newsFeed_item',
                'title_selector': '.newsFeed_item_title',
                'category': 'ç·åˆ'
            },
            'livedoor': {
                'url': 'http://news.livedoor.com/ranking/',
                'ranking_selector': '.rankingList li',
                'title_selector': 'a',
                'category': 'ç·åˆ'
            },
            'oricon': {
                'url': 'https://www.oricon.co.jp/news/rank/',
                'ranking_selector': '.ranking-box li',
                'title_selector': 'a',
                'category': 'ã‚¨ãƒ³ã‚¿ãƒ¡'
            },
            'modelpress': {
                'url': 'https://mdpr.jp/ranking',
                'ranking_selector': '.p-ranking__item',
                'title_selector': '.p-ranking__title',
                'category': 'ã‚¨ãƒ³ã‚¿ãƒ¡'
            },
            'itmedia': {
                'url': 'https://www.itmedia.co.jp/news/ranking/',
                'ranking_selector': '.colBoxRanking li',
                'title_selector': 'a',
                'category': 'IT'
            },
            'nlab': {
                'url': 'https://nlab.itmedia.co.jp/nl/subtop/ranking.html',
                'ranking_selector': '.c-ranking__item',
                'title_selector': '.c-ranking__title',
                'category': 'ãƒãƒƒãƒˆ'
            }
        }
        
        self.pattern_analyzer = PatternAnalyzer()
        self.ranking_history = []
        
        if httpx:
            self.client = httpx.Client(
                timeout=30.0,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            )
        else:
            self.client = None
    
    def collect_all_rankings(self):
        """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’åé›†"""
        all_rankings = {}
        
        # httpxãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        if not self.client or not BeautifulSoup:
            logger.warning("httpx or BeautifulSoup not available, using dummy data")
            return self._get_dummy_rankings()
        
        for site_name, config in self.news_sites.items():
            try:
                logger.info(f"Collecting rankings from {site_name}")
                rankings = self.scrape_ranking(site_name, config)
                all_rankings[site_name] = rankings
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                if rankings:
                    self.pattern_analyzer.analyze_titles(rankings, config['category'])
                
            except Exception as e:
                logger.error(f"Error scraping {site_name}: {e}")
                continue
        
        # å±¥æ­´ã«ä¿å­˜
        self.ranking_history.append({
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'rankings': all_rankings
        })
        
        return all_rankings
    
    def scrape_ranking(self, site_name, config):
        """å€‹åˆ¥ã‚µã‚¤ãƒˆã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            response = self.client.get(config['url'])
            if response.status_code != 200:
                logger.error(f"Failed to fetch {site_name}: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            rankings = []
            items = soup.select(config['ranking_selector'])[:20]  # TOP20
            
            for i, item in enumerate(items):
                title_elem = item.select_one(config['title_selector'])
                if title_elem:
                    title_text = title_elem.text.strip()
                    rankings.append({
                        'rank': i + 1,
                        'title': title_text,
                        'url': title_elem.get('href', ''),
                        'category': config['category']
                    })
            
            return rankings
            
        except Exception as e:
            logger.error(f"Error in scrape_ranking for {site_name}: {e}")
            return []
    
    def _get_dummy_rankings(self):
        """ãƒ€ãƒŸãƒ¼ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        dummy_titles = {
            'yahoo_news': [
                "ã€é€Ÿå ±ã€‘æœ‰åä¿³å„ªAãŒé›»æ’ƒçµå©šï¼ãŠç›¸æ‰‹ã¯20ä»£ãƒ¢ãƒ‡ãƒ«",
                "ã€è¡æ’ƒã€‘äººæ°—ã‚¢ã‚¤ãƒ‰ãƒ«BãŒæ¶™ã®å‘Šç™½ã€Œã‚‚ã†é™ç•Œã§ã—ãŸã€",
                "æ”¿åºœãŒæ–°ãŸãªçµŒæ¸ˆå¯¾ç­–ã‚’ç™ºè¡¨ã€ç·é¡30å…†å††è¦æ¨¡",
                "ã€ç‹¬å ã€‘å¤§ç‰©èŠ¸äººCã®ä¸å€«ç–‘æƒ‘ã€æœ¬äººãŒæ¿€ç™½",
                "ãƒ—ãƒ­é‡çƒé¸æ‰‹DãŒå¹´ä¿¸5å„„å††ã§å¥‘ç´„æ›´æ”¹"
            ],
            'oricon': [
                "ã€å†™çœŸ20æšã€‘ç¾äººå¥³å„ªEã®ã™ã£ã´ã‚“å§¿ãŒè©±é¡Œ",
                "äººæ°—ä¿³å„ªFãŒç†±æ„›ç™ºè¦šï¼ãƒ‡ãƒ¼ãƒˆç¾å ´ã‚’æ¿€å†™",
                "ã€å‹•ç”»ã€‘ã‚¢ã‚¤ãƒ‰ãƒ«Gã®æ–°æ›²MVãŒ1000ä¸‡å›å†ç”Ÿçªç ´",
                "å¤§ç‰©æ­Œæ‰‹HãŒæ´»å‹•ä¼‘æ­¢ã‚’ç™ºè¡¨ã€ãƒ•ã‚¡ãƒ³é¨’ç„¶",
                "ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘ä»Šå¹´æœ€ã‚‚è¼ã„ãŸä¿³å„ªTOP10"
            ],
            'itmedia': [
                "ã€é€Ÿå ±ã€‘æ–°å‹iPhoneç™ºè¡¨ã€ä¾¡æ ¼ã¯20ä¸‡å††è¶…ãˆã‹",
                "AIæŠ€è¡“ã§å¹´å1000ä¸‡å††è¶…ãˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒæ€¥å¢—",
                "ã€è§£èª¬ã€‘è©±é¡Œã®ChatGPTæœ€æ–°ç‰ˆã€ä½•ãŒå¤‰ã‚ã£ãŸï¼Ÿ",
                "å¤§æ‰‹ITä¼æ¥­ãŒå¤§è¦æ¨¡ãƒªã‚¹ãƒˆãƒ©ã‚’ç™ºè¡¨",
                "ã€æ¯”è¼ƒã€‘æœ€æ–°ã‚¹ãƒãƒ›æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°TOP5"
            ]
        }
        
        all_rankings = {}
        for site, titles in dummy_titles.items():
            rankings = []
            for i, title in enumerate(titles):
                rankings.append({
                    'rank': i + 1,
                    'title': title,
                    'url': f"https://example.com/{site}/{i+1}",
                    'category': self.news_sites.get(site, {}).get('category', 'ç·åˆ')
                })
            all_rankings[site] = rankings
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            self.pattern_analyzer.analyze_titles(
                rankings, 
                self.news_sites.get(site, {}).get('category', 'ç·åˆ')
            )
        
        return all_rankings
    
    def identify_viral_patterns(self):
        """ãƒã‚¤ãƒ©ãƒ«ã«ãªã‚Šã‚„ã™ã„è¨˜äº‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š"""
        patterns = self.pattern_analyzer.get_viral_patterns()
        
        return {
            'hot_keywords': patterns['keywords'],
            'title_patterns': patterns['patterns'],
            'optimal_length': patterns['length'],
            'emotion_triggers': patterns['emotions']
        }
    
    def save_analysis_data(self):
        """åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            data_dir = Path('.')
            analysis_file = data_dir / 'ranking_analysis.json'
            
            analysis_data = {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'rankings': self.ranking_history[-1]['rankings'] if self.ranking_history else {},
                'viral_patterns': self.identify_viral_patterns(),
                'top_trends': self.pattern_analyzer.get_top_trends()
            }
            
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Analysis data saved to {analysis_file}")
            return True
            
        except Exception as e:
            logger.error(f"Error saving analysis data: {e}")
            return False
    
    def close(self):
        """Close HTTP client"""
        if self.client:
            self.client.close()


class PatternAnalyzer:
    def __init__(self):
        self.title_database = []
        self.keyword_frequency = {}
        self.successful_patterns = []
        
    def analyze_titles(self, rankings, category):
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        for item in rankings[:10]:  # TOP10ã®ã¿åˆ†æ
            title = item['title']
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = self.extract_keywords(title)
            for keyword in keywords:
                key = f"{category}:{keyword}"
                self.keyword_frequency[key] = self.keyword_frequency.get(key, 0) + (11 - item['rank'])
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
            patterns = self.extract_pattern(title)
            self.successful_patterns.append({
                'pattern': patterns,
                'category': category,
                'rank': item['rank'],
                'original': title
            })
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
            self.title_database.append({
                'title': title,
                'category': category,
                'rank': item['rank'],
                'keywords': keywords,
                'patterns': patterns,
                'length': len(title)
            })
    
    def extract_keywords(self, title):
        """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # é«˜é »åº¦ã§å‡ºç¾ã™ã‚‹ç…½ã‚Šãƒ¯ãƒ¼ãƒ‰
        hot_words = [
            'è¡æ’ƒ', 'é€Ÿå ±', 'ç·Šæ€¥', 'ç™ºè¦š', 'æ¿€ç™½', 'å‘Šç™½', 'æš´éœ²',
            'ç‚ä¸Š', 'æ‰¹åˆ¤', 'åè«–', 'æ¿€æ€’', 'å·æ³£', 'æ¶™',
            'çµå©š', 'é›¢å©š', 'ç ´å±€', 'ç†±æ„›', 'ä¸å€«', 'ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«',
            'é€®æ•', 'æ›¸é¡é€æ¤œ', 'èµ·è¨´', 'åˆ¤æ±º',
            'å¼•é€€', 'å’æ¥­', 'æ´»å‹•ä¼‘æ­¢', 'è§£æ•£',
            'åˆ', 'æœ€å¾Œ', 'é™å®š', 'ç‹¬å ', 'ã‚¹ã‚¯ãƒ¼ãƒ—',
            'ãƒ¤ãƒã„', 'ãƒ¤ãƒã™ã', 'ç¥', 'æœ€å¼·', 'æœ€æ‚ª',
            '1ä½', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'TOP', 'æœ€æ–°',
            'ç¾äºº', 'ã‚¤ã‚±ãƒ¡ãƒ³', 'ã‹ã‚ã„ã„', 'ã‚»ã‚¯ã‚·ãƒ¼',
            'å¹´å', 'å„„', 'ä¸‡å††', 'çµ¦æ–™', 'è³‡ç”£',
            'æ¿€å¤‰', 'å¤‰è²Œ', 'æ¿€å¤ªã‚Š', 'æ¿€ã‚„ã›', 'æ•´å½¢',
            'ã™ã£ã´ã‚“', 'ç§æœ', 'ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆ', 'å¯†ç€',
            'å†™çœŸ', 'å‹•ç”»', 'ç”»åƒ', 'æ¿€å†™',
            'ç™ºè¡¨', 'æ±ºå®š', 'åˆ¤æ˜', 'ç¢ºå®š',
            'ç†ç”±', 'çœŸç›¸', 'æœ¬éŸ³', 'è£å´',
            'æ‚²å ±', 'æœ—å ±', 'è¨ƒå ±', 'å‰å ±'
        ]
        
        found_keywords = []
        for word in hot_words:
            if word in title:
                found_keywords.append(word)
        
        return found_keywords
    
    def extract_pattern(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ã®æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        patterns = []
        
        # ã€ã€‘ã®ä½¿ç”¨
        if 'ã€' in title and 'ã€‘' in title:
            patterns.append('bracket_emphasis')
        
        # ã€Œã€ã®ä½¿ç”¨
        if 'ã€Œ' in title and 'ã€' in title:
            patterns.append('quote_usage')
        
        # æ•°å­—ã®ä½¿ç”¨
        if re.search(r'\d+', title):
            patterns.append('number_usage')
        
        # ç–‘å•å½¢
        if 'ï¼Ÿ' in title or '?' in title or (title and title[-1] == 'ã‹'):
            patterns.append('question_form')
        
        # æ„Ÿå˜†ç¬¦
        if 'ï¼' in title or '!' in title or '!!' in title:
            patterns.append('exclamation')
        
        # çœç•¥å½¢ï¼ˆ...ã€ãªã©ï¼‰
        if 'â€¦' in title or '...' in title:
            patterns.append('ellipsis')
        
        # å†™çœŸãƒ»å‹•ç”»ã‚¢ãƒ”ãƒ¼ãƒ«
        if 'å†™çœŸ' in title or 'ç”»åƒ' in title or 'å‹•ç”»' in title:
            patterns.append('visual_content')
        
        # äººåã®ä½¿ç”¨ï¼ˆAã€Bã€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼‰
        if re.search(r'[A-Z](?:ã•ã‚“|æ°|å®¹ç–‘è€…|è¢«å‘Š)?', title):
            patterns.append('anonymous_person')
        
        return patterns
    
    def get_viral_patterns(self):
        """ãƒã‚¤ãƒ©ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é›†è¨ˆ"""
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        top_keywords = sorted(
            self.keyword_frequency.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:50]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
        pattern_stats = {}
        for item in self.successful_patterns:
            for pattern in item['pattern']:
                if pattern not in pattern_stats:
                    pattern_stats[pattern] = 0
                pattern_stats[pattern] += (11 - item['rank'])
        
        return {
            'keywords': top_keywords,
            'patterns': pattern_stats,
            'length': self.analyze_title_length(),
            'emotions': self.analyze_emotions()
        }
    
    def analyze_title_length(self):
        """æœ€é©ãªã‚¿ã‚¤ãƒˆãƒ«é•·ã‚’åˆ†æ"""
        if not self.title_database:
            return {'min': 30, 'max': 50, 'optimal': 40}
        
        lengths = [item['length'] for item in self.title_database if item['rank'] <= 5]
        if lengths:
            return {
                'min': min(lengths),
                'max': max(lengths),
                'optimal': sum(lengths) // len(lengths)
            }
        return {'min': 30, 'max': 50, 'optimal': 40}
    
    def analyze_emotions(self):
        """æ„Ÿæƒ…ãƒˆãƒªã‚¬ãƒ¼ã‚’åˆ†æ"""
        emotions = {
            'anger': ['æ¿€æ€’', 'æ‰¹åˆ¤', 'ç‚ä¸Š', 'æš´è¨€', 'å¤±è¨€', 'åè«–', 'æŠ—è­°'],
            'sadness': ['æ¶™', 'å·æ³£', 'æ‚²å ±', 'è¨ƒå ±', 'å¼•é€€', 'å’æ¥­', 'æ‚²ã—ã„'],
            'surprise': ['è¡æ’ƒ', 'é©šæ„•', 'ã¾ã•ã‹', 'æ„å¤–', 'æ€¥å±•é–‹', 'é€Ÿå ±', 'ã³ã£ãã‚Š'],
            'joy': ['æœ—å ±', 'çµå©š', 'å¦Šå¨ ', 'å¾©æ´»', 'å¿«æŒ™', 'ç¥ç¦', 'å¹¸ã›'],
            'fear': ['ææ€–', 'å±é™º', 'è­¦å‘Š', 'æ³¨æ„', 'è¢«å®³', 'äº‹æ•…', 'æ€–ã„'],
            'disgust': ['æœ€æ‚ª', 'é…·ã„', 'æ‰¹åˆ¤æ®ºåˆ°', 'å¤§ç‚ä¸Š', 'å¤±æœ›', 'å¹»æ»…']
        }
        
        emotion_scores = {}
        for emotion, words in emotions.items():
            score = 0
            for word in words:
                for category in ['ç·åˆ', 'ã‚¨ãƒ³ã‚¿ãƒ¡', 'ã‚¹ãƒãƒ¼ãƒ„', 'ç¤¾ä¼š']:
                    score += self.keyword_frequency.get(f"{category}:{word}", 0)
            emotion_scores[emotion] = score
        
        return emotion_scores
    
    def get_top_trends(self):
        """ç¾åœ¨ã®ãƒˆãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—"""
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        category_trends = {}
        for key, score in self.keyword_frequency.items():
            category, keyword = key.split(':', 1)
            if category not in category_trends:
                category_trends[category] = []
            category_trends[category].append((keyword, score))
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒˆãƒƒãƒ—5
        for category in category_trends:
            category_trends[category] = sorted(
                category_trends[category], 
                key=lambda x: x[1], 
                reverse=True
            )[:5]
        
        return category_trends


def main():
    """Main execution function"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    try:
        analyzer = NewsRankingAnalyzer()
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°åé›†
        logger.info("ğŸ“Š Collecting news rankings...")
        rankings = analyzer.collect_all_rankings()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        logger.info("ğŸ” Analyzing viral patterns...")
        patterns = analyzer.identify_viral_patterns()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        analyzer.save_analysis_data()
        
        # çµæœè¡¨ç¤º
        print("\nğŸ”¥ Top Viral Keywords:")
        for keyword, score in patterns['hot_keywords'][:10]:
            print(f"  {keyword}: {score}ç‚¹")
        
        print("\nğŸ“Š Title Pattern Statistics:")
        for pattern, score in sorted(patterns['title_patterns'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {pattern}: {score}ç‚¹")
        
        print("\nğŸ˜Š Emotion Triggers:")
        for emotion, score in sorted(patterns['emotion_triggers'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {emotion}: {score}ç‚¹")
        
        print("\nâœ… Analysis completed successfully!")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal error: {str(e)}")
        raise
    finally:
        if 'analyzer' in locals():
            analyzer.close()


if __name__ == "__main__":
    main()